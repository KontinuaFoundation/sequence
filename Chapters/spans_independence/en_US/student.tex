\chapter{Vector Spans and Independence}

A vector span is the collection of vectors obtained by scaling and combining the original set of vectors in all possible proportions.  Formally, if the set $S = \{v_1, v_2, ..., v_n\}$ contains vectors from a vector space $V$, then the span of $S$ is given by:

\begin{equation}
{Span}(S) = \{a_1v_1 + a_2v_2 + ... + a_nv_n : a_1, a_2, ..., a_n \in \mathbb{R}\}
\end{equation}

This means that any vector in the Span$(S)$ can be written as a linear combination of the vectors in $S$.

Vector spans have practical applications in a number of fields. Computer graphics and physics are only two of them. For example, in space travel, knowing the vector span is essential to calculating a slingshot maneuver that will give spacecraft a gravity boost from a planet. For this, you'd need to know the gravity vector of the planet relative to the sun and the velocity vectors that characterize the spacecraft. Engineers would use this information to figure out the trajectory angle that would allow the spacecraft to achieve a particular velocity in the desired direction. 

For a discussion of linear combinations and vector span, view this Khan Academy video: https://rb.gy/g1snk

\section{Vector Independence}
A set of vectors $S = \{v_1, v_2, ..., v_n\}$ is  linearly independent if the only solution to the equation $a_1v_1 + a_2v_2 + ... + a_nv_n = 0$.

is $a_1 = a_2 = ... = a_n = 0$. This means that no vector in the set can be written as a linear combination of the other vectors.

If there exists a nontrivial solution (i.e., a solution where some $a_i \neq 0$), then the vectors are said to be linearly dependent. This means that at least one vector in the set can be written as a linear combination of the other vectors.

The concept of vector independence is fundamental to the study of vector spaces, bases, and rank. You'll learn more about these concepts in future modules. 

\subsection{Dependent Vectors}
Let's start by looking at two vectors. 

$$v_1 = \begin{bmatrix}
			2 \\
			4
		\end{bmatrix}$$
$$v_2 = \begin{bmatrix}
			-14 \\
			-28
\end{bmatrix}$$

These two vectors are dependent because $v_2 = -7*v_2$. This is an obvious example but let's show it mathematically. If linearly independent, the two vectors must satisfy:

	$$v_1a_1 + v_1a_2 = 0$$
	$$v_2a_1 + v_2a_2 = 0$$

which is:
	$$2a_1 -14a_2 = 0$$
	$$4a_1 -28a_2 = 0$$

To solve, multiply the top equation by -2 and add it to the bottom: 
$$2a_1 -14a_2 = 0 $$
$$ 0  + 0     = 0 $$

The bottom equation drops out. Now  olve for $a_1$ in the remaining equation:
$$a_1 = -7a_2$$
As you can see, one vector is a multiple of another. $$a_1 \neq a_2 \neq 0$$

\subsection{Independent Vectors}
Let's see if these two vectors are independent.
$$v_1 = \begin{bmatrix}
1 \\
0
\end{bmatrix}$$
$$v_2 = \begin{bmatrix}
0 \\
-1
\end{bmatrix}$$

To be independent,the two vectors must satisfy:
	$$v_1a_1 + v_1a_2 = 0$$
	$$v_2a_1 + v_2a_2 = 0$$
	
which is:
$$\begin{bmatrix}
	a_1 + 0*a_2 \\
	0*a_1 + a_2
\end{bmatrix}$$

So:
$a_1 = a_2 = 0$
These vectors are not only independent, but they are orthogonal (perpendicular) to one another. You'll learn more about orthogonality later.

Here is an example whose solution isn't as obvious. You can solve using Gaussian elmination.
$$v_1 = [2,1]$$
$$v_2 = [1,-6]$$

Rewrite as a system of equations:
$$
	a_1*2 + a_2*1 = 0 
	a_1*1 + a_2*(-6) = 0
$$
First swap the equations to that the the top equation has a coefficient of 1 for $a_1$:
$$
	a_1 - 6a_2 = 0 
	2a_1 + a_2 = 0 
$$
Next multiply row 1 by -2 and add it to row 2:
$$
	a_1 - 6a_2 = 0 
	0  - 11a_2 = 0 
$$
Multiply row 2 by 1 divided by 11.
$$
	a_1 - 6a_2 = 0 
	0  +  a_2 = 0 
$$
Back substitute $a_2$ solution into the first equation:
$$
	a_1  = 0 
	a_2 = 0 
$$
Therefore $$a_1 = a_2 = 0$$ and the two vectors are linearly independent.

\begin{Exercise}[title={Vector Independence}, label=vector_independence]
    Are these vectors independent? 
    \begin{itemize}
    	\item $[2, 1, 4]$
    	\item $[2, -1, 2]$ 
    	\item $[0, 1, -2]$
    \end{itemize}
    Show your work.
\end{Exercise}
\begin{Answer}[ref=vector_independence]
    Rewrite as a system of equations:
        $$\begin{matrix}
			2*a_1 +2*a_2 + 0*a_3 = 0 \\
			1*a_1 - 1*a_2 +1*a_3 = 0 \\
			4*a_1 + 2*a_2 - 2*a_3 = 0
		  \end{matrix} $$
	Simplify
		$$\begin{matrix}
			2a_1 +2*a_2  = 0 \\
			a_1 - a_2 + a_3 = 0 \\
			4a_1 + 2a_2 - 2a_3 = 0
		  \end{matrix} $$
	Swap row 2 and 1:
		$$\begin{matrix}
			a_1 - a_2 + a_3 = 0 \\
			2a_1 +2*a_2  = 0 \\
			4a_1 + 2a_2 - 2a_3 = 0
		  \end{matrix} $$
	Multiply row 1 by -2 and add to row 2:
	   $$\begin{matrix}
			a_1 - a_2 + a_3 = 0 \\
			0 +  3*a_2 -2a_3    = 0 \\
			4a_1 + 2a_2 - 2a_3 = 0
		  \end{matrix} $$
	Multiply row 1 by -4 and add to row 3:	
	    $$\begin{matrix}
			a_1 - a_2 + a_3 = 0 \\
			0 +   3*a_2 -2a_3    = 0 \\
			0   + 6a_2 - 6a_3 = 0
		  \end{matrix} $$
	Multiply row 2 by -4 and add to row 3:
	   $$\begin{matrix}
			a_1 - a_2 + a_3 = 0 \\
			0 +   3*a_2 -2a_3    = 0 \\
			0   + 0   - 2a_3 = 0
		  \end{matrix} $$
	Multiply row 3 by -1 and add to row 2:
		$$\begin{matrix}
			a_1 - a_2 + a_3 = 0 \\
			0 +   3*a_2 +0    = 0 \\
			0   + 0   - 2a_3 = 0
		\end{matrix} $$
    Divide row 3 by -2 and row 2 by $\frac{1}{3}$:
    	$$\begin{matrix}
			a_1 - a_2 + a_3 = 0 \\
			0 +   a_2 +0    = 0 \\
			0   + 0   a_3 = 0
		\end{matrix} $$
	Backsubstitute $a_2$ and $a_3$ into row 1:
	 	$$\begin{matrix}
			a_1 +0 + ) = 0 \\
			0 +   a_2 +0   = 0 \\
			0   + 0   a_3 = 0
		\end{matrix} $$
	 Therefore $$a_1 = a_2 = a_3 = 0$$.
\end{Answer}
    
\section{Checking for Linear Independence Using Python}  
One way to use python to check for linear independence is to use the linalg.solve() function to solve the system of equations. You need to create an array that contains the coefficients of the variable and a vector that contains the values on the right-side of each equation. So far, you've either been given equations that equal 0 or you've manipulated each equation to be equal to 0. 

Let's first see how to use python to solve the equations in the previous exercise. If the equations are linearly independent, then $a_1 = a_2 = a_3 = 0$

Create a file called linear\_independence.Python and enter this code:
\begin{Verbatim}
import numpy as np

A = np.array([[ 2, 2, 0], 
              [ 1, -1,1],
              [4, 2, -2]])
b = np.array([0,0,0])
c = np.linalg.solve(A,b)
print(c)
\end{Verbatim}
Your should get this result, which shows the equations are linearly independent.
\begin{Verbatim}
[ 0. -0.  0.]
\end{Verbatim}
But what happens if the equations are not independent? Let's make the first two equations dependent by making equation 1 two times equation 2. Enter this code into your file:
\begin{Verbatim}
import numpy as np

D = np.array([[ 2, -2, 2], 
              [ 1, -1,1],
              [4, 2, -2]])
e = np.array([0,0,0])
f = np.linalg.solve(D,f)
print(f)
\end{Verbatim}
You should get many lines indicating an error. Among the spew, you should see:
\begin{Verbatim}
raise LinAlgError("Singular matrix")
\end{Verbatim}
So while the linalg.solve() function is quite useful for solving a system of independent linear equations, it is not the most elegant way to figure out if the equations are dependent. In the next module you will learn concepts and techniques that allow you to first check for independence prior to solving a set of equations. 

For now, let's use the  linalg.solve() function to find a solution for a set of equations known to be linearly independent.
$$4x_1 + 3x_2 - 5x_3 = 2$$
$$-2x_1- 4x_2 - 5x_3 = 5$$
$$       8x_2 + 8x_3  = -3$$
You will create a matrix that contains all the coeffients and an array that contains the values on the right-side of the equations. 

Enter this code into your file. 
\begin{Verbatim}
G = np.array([[4, 3, -5], 
              [-2, -4, 5], 
              [8, 8, 0]])
h = np.array([2, 5, -3])

j = np.linalg.solve(G, h)
print(j)
\end{Verbatim}
You should get this answer:
\begin{Verbatim}
[ 2.20833333 -2.58333333 -0.18333333]
\end{Verbatim}

