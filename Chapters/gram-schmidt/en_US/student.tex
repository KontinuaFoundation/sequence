\chapter{The Gram-Schmidt Process}

The Gram-Schmidt process is a method use to transform a set of linearly independent vectors to a set of orthogonal (perpendicular) vectors. The original vectors and the transformed vectors span the same subspace. 

The process was named after two mathematicians: JÃ¸rgen Pedersen Gram, a Danish actuary mathematician, and Erhard Schmidt, a German mathematician. The men developed the orthogonalization process independently. Gram introduced the process in 1883, whereas Schmidt did his work in 1907.  It was not named the Gram-Schmidt process until sometime later, after both mathematicians became well known in the mathematical community.

In the last chapter, you learned how to calculate a projection of one vector on another. Given two vectors, $u$ and $v$, the projection separates $u$ into the part that is orthogonal to $v$ and the part that has a dependency with $v$. The Gram-Schmidt process builds on the notion of projections to iteritively strip away dependencies between vectors until the vectors that remain are orthogonal. If there happens to be a vector that is a linear combination of the others, that vector will be reduced to the zero vector. The vectors that remain define a new basis for space spanned by the original set of vectors. 

Gram-Schmidt has many practical applications in science and engineering, such as:
\begin{enumerate}
\item In signal processing, it can represent an audio signal with fewer components making it easier to isolate and remove noise. 
\item In statistics and data analysis, it can reduce the complexity of a dataset so that it is easier to see which aspects or features contribute to the analysis. 
\end{enumerate}

\section{The Process}

The Gram-Schmidt process orthonormalizes a set of vectors in an inner product space, most commonly the Euclidean space
$\mathbb{R}^n$. The process takes a finite, linearly independent set
$S = \{v_1, v_2, \ldots, v_k\}$ for $k \leq n$, and generates an
orthogonal set $S' = \{u_1, u_2, \ldots, u_k\}$ that spans the same
k-dimensional subspace of $\mathbb{R}^n$ as $S$.\index{Gram-Schmidt process}

Let's look at how the process works. Given a set of vectors $S = \{v_1, v_2, \ldots, v_k\}$, the Gram-Schmidt process is as follows:

\begin{enumerate}
    \item Let $u_1 = v_1$.
    \item For $j = 2, 3, \ldots, k$:
    \begin{enumerate}
        \item Let $w_j = v_j - \sum_{i=1}^{j-1} \frac{\langle v_j, u_i \rangle}{\langle u_i, u_i \rangle} u_i$
        \item Let $u_j = w_j$
    \end{enumerate}
\end{enumerate}

Here, $\langle . , . \rangle$ denotes the inner product. 

The set of vectors $S' = \{u_1, u_2, \ldots, u_k\}$ obtained from this
process is orthogonal, but not necessarily orthonormal. To create
an orthonormal set, you simply need to normalize each vector $u_i$ to
unit length. That is, $u_i' = \frac{u_i}{\|u_i\|}$, where $\|.\|$
denotes the norm (or length) of a vector.

Among other things, making vectors orthonormal simplifies calculations makes it easier to define rotations and transformations, and provides a framework for calculations in fields such as quantum mechanics.

\section{Example Calculation}

Given a set of linearly independent vectors, we will use the Gram-Schmidt process to find an orthogonal basis.

Let $$W = Span (x_1, x_2, x_3)$$ where
$$x_1 = (1, 2, -2) $$
$$x_2 = (1,0,-4) $$
$$x_3 = (5,2,0)$$

The three orthogonal vectors will define the same subspace as the original vectors. 

The first vector of the orthogonal subspace is easy to define. We set it to be the same as $x_1$.

$$v_1 = x_1 = (1, 2, -2)$$

The second orthogonal vector is a projection of $x_2$ onto $v_1$. You learned projections in the last chapter, so this should be fairly straightforward.

$$v_2 = x_2 - \frac{x_2v_1}{v_1v_1} v_1$$

Substitute the values:

$$v_2 = (1, 0, -4)  - \frac{(1,0, -4)(1,2,-2)}{(1,2,-2)(1,2,-2)} \ (1, 2, -2) $$

Calculate the coefficient for $v_1$:
$$v_2 = (1, 0, -4) - \frac{9}{9} (1, 2, -2)  \\$$
Perform the subtraction:
$$v_2 = (0, -2, -2) \\$$
 
The third vector for the orthogonal subspace is a projection onto $v_1$ and $v_2$. 
$$v_3 = x_3 - \frac{x_3v_1}{v_1v_1} v_1 - \frac{x_3v_2}{v_2v_2} v_2$$

Substitute the values:
$$v_3 = (5,2,0) - \frac{(5,2,0)(1,2,-2)}{(1,2,-2)(1,2,-2)} (1,2,-2) - \frac{(5,2,0)(1,0,-4)}{(1,0,-4)(1,0,-4)} (1,0,-4)$$
$$v_3 =  (5,2,0)  - (9/9) (1,2,-2) -  (-4/8)(1,0,-4)$$
$$v_3 =  (5,2,0) - (1,2,-2) + (1/2)(1,0,-4)$$
$$v_3  = (5,2,0 ) - (1,2,-2) + (0,-1,-1)$$
$$v_3 = (4, -1, 1)$$

This set of vectors is orthogonal, so we need to normalize them so that the vectors are orthonormal. Recall that an orthonormal vector has a length of 1 and is computed using this formula:
$$normalizedVector = vector / np.sqrt(np.sum(vector**2))$$
Thus the normalized set of vectors is:
$$v_1 = (0.33,  0.67, -0.67)$$
$$v_2 = (0.0,  -0.71, -0.71 )$$
$$v_3 = (0.94, -0.24,  0.24 ) $$

\begin{Exercise}[title={Gram-Schmidt Process}, label=gram_schmidt]
    Use the Gram-Schmidt process to to find an orthogonal basis for the span defined by $x_1, x_2$ where:
    $$x_1 = (1, 1, 1)$$
	$$x_2 = (0, 1, 1) $$
\end{Exercise}
\begin{Answer}[ref=gram_schmidt]
      The first vector of the orthogonal subspace is: 
		$$v_1 = x_1 =  (1, 1, 1) $$ 
	  The second vector of the subspace is a projection of $x_2$ onto $v_1$.
	  $$v_2 = x_2 - \frac{x_2v_1}{v_1v_1} v_1$$
	  Substitute the values:
	  $$v_2 =  (0, 1, 1)  - \frac{(0,1, 1)(1,1,1)}{(1,1,1)(1,1,-1)}  (1, 1, 1)$$
	  $$v_2 =  (0, 1, 1)  - (2/3) (1, 1, 1) $$
	  $$v_2 =  (-2/3, 1/3, 1/3)$$
	  Normalize:
	  $$v_1 = v_1/\sqrt{|v_1|} $$
	  $$v_1 = (1, 1, 1 )/\sqrt{|v_1|} $$
	  $$v_1 = (0.577, 0.577, 0.577)  $$  
	  $$v_2 = v_2/\sqrt{|v_2|} $$
	  $$v_2 = (0, 1, 1) \sqrt{|v_2|} $$
	  $$v_2 =  (-0.816,  0.408,  0.408) $$
\end{Answer}

\section{The Gram-Schmidt Process in Python}
Create a file called \filename{vectors\_gram-schmidt.py} and enter this code:
\begin{Verbatim}
# import numpy to perform operations on vector
import numpy as np

# Find an orthogonal basis for the span of these three vectors  
x1 = np.array([1, 2, -2]) 
x2 = np.array([1, 0, -4]) 
x3 = np.array([5, 2, 0])    
   
# v1 = x1
v1 = x1
print("v1 = ",v1)

# v2 = x2 - (the projection of x2 on v1)
v2 = x2 - (np.dot(x2,v1)/np.dot(v1,v1))*v1
print("v2 = ", v2)

# v3 = x3 - (the projection of x3 on v1) - (the projection of x3 on v3)
v3 = x3 - (np.dot(x3,v1)/np.dot(v1,v1))*v1 - (np.dot(x3,v2)/np.dot(v2,v2))*v2
print("v3 =", v3)

# Next, normalize each vector to get a set of vectors that is both orthogonal and orthonormal:
v1_norm = v1 / np.sqrt(np.sum(v1**2))
v2_norm = v2 / np.sqrt(np.sum(v2**2))
v3_norm = v3 / np.sqrt(np.sum(v3**2))
print("v1_norm = ", v1_norm)
print("v2_norm = ", v2_norm)
print("v3_norm = ", v3_norm)
\end{Verbatim}

\section{Where to Learn More}
Watch this video from Khan Academy about the Gram-Schmidt process: \url{https://www.khanacademy.org/math/linear-algebra/alternate-bases/orthonormal-basis/v/linear-algebra-the-gram-schmidt-process}
